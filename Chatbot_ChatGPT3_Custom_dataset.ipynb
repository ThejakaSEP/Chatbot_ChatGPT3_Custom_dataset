{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Download the data for your custom knowledge base"
      ],
      "metadata": {
        "id": "8CbC2G1RQcQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ThejakaSEP/Chatbot_ChatGPT3_Custom_dataset.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhKGxpsRR-T0",
        "outputId": "1ec78f13-9955-40b9-ed43-97ea3ce24411"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chatbot_ChatGPT3_Custom_dataset'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/30)\u001b[K\rremote: Counting objects:   6% (2/30)\u001b[K\rremote: Counting objects:  10% (3/30)\u001b[K\rremote: Counting objects:  13% (4/30)\u001b[K\rremote: Counting objects:  16% (5/30)\u001b[K\rremote: Counting objects:  20% (6/30)\u001b[K\rremote: Counting objects:  23% (7/30)\u001b[K\rremote: Counting objects:  26% (8/30)\u001b[K\rremote: Counting objects:  30% (9/30)\u001b[K\rremote: Counting objects:  33% (10/30)\u001b[K\rremote: Counting objects:  36% (11/30)\u001b[K\rremote: Counting objects:  40% (12/30)\u001b[K\rremote: Counting objects:  43% (13/30)\u001b[K\rremote: Counting objects:  46% (14/30)\u001b[K\rremote: Counting objects:  50% (15/30)\u001b[K\rremote: Counting objects:  53% (16/30)\u001b[K\rremote: Counting objects:  56% (17/30)\u001b[K\rremote: Counting objects:  60% (18/30)\u001b[K\rremote: Counting objects:  63% (19/30)\u001b[K\rremote: Counting objects:  66% (20/30)\u001b[K\rremote: Counting objects:  70% (21/30)\u001b[K\rremote: Counting objects:  73% (22/30)\u001b[K\rremote: Counting objects:  76% (23/30)\u001b[K\rremote: Counting objects:  80% (24/30)\u001b[K\rremote: Counting objects:  83% (25/30)\u001b[K\rremote: Counting objects:  86% (26/30)\u001b[K\rremote: Counting objects:  90% (27/30)\u001b[K\rremote: Counting objects:  93% (28/30)\u001b[K\rremote: Counting objects:  96% (29/30)\u001b[K\rremote: Counting objects: 100% (30/30)\u001b[K\rremote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects:   6% (1/16)\u001b[K\rremote: Compressing objects:  12% (2/16)\u001b[K\rremote: Compressing objects:  18% (3/16)\u001b[K\rremote: Compressing objects:  25% (4/16)\u001b[K\rremote: Compressing objects:  31% (5/16)\u001b[K\rremote: Compressing objects:  37% (6/16)\u001b[K\rremote: Compressing objects:  43% (7/16)\u001b[K\rremote: Compressing objects:  50% (8/16)\u001b[K\rremote: Compressing objects:  56% (9/16)\u001b[K\rremote: Compressing objects:  62% (10/16)\u001b[K\rremote: Compressing objects:  68% (11/16)\u001b[K\rremote: Compressing objects:  75% (12/16)\u001b[K\rremote: Compressing objects:  81% (13/16)\u001b[K\rremote: Compressing objects:  87% (14/16)\u001b[K\rremote: Compressing objects:  93% (15/16)\u001b[K\rremote: Compressing objects: 100% (16/16)\u001b[K\rremote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 30 (delta 13), reused 30 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects:   3% (1/30)\rUnpacking objects:   6% (2/30)\rUnpacking objects:  10% (3/30)\rUnpacking objects:  13% (4/30)\rUnpacking objects:  16% (5/30)\rUnpacking objects:  20% (6/30)\rUnpacking objects:  23% (7/30)\rUnpacking objects:  26% (8/30)\rUnpacking objects:  30% (9/30)\rUnpacking objects:  33% (10/30)\rUnpacking objects:  36% (11/30)\rUnpacking objects:  40% (12/30)\rUnpacking objects:  43% (13/30)\rUnpacking objects:  46% (14/30)\rUnpacking objects:  50% (15/30)\rUnpacking objects:  53% (16/30)\rUnpacking objects:  56% (17/30)\rUnpacking objects:  60% (18/30)\rUnpacking objects:  63% (19/30)\rUnpacking objects:  66% (20/30)\rUnpacking objects:  70% (21/30)\rUnpacking objects:  73% (22/30)\rUnpacking objects:  76% (23/30)\rUnpacking objects:  80% (24/30)\rUnpacking objects:  83% (25/30)\rUnpacking objects:  86% (26/30)\rUnpacking objects:  90% (27/30)\rUnpacking objects:  93% (28/30)\rUnpacking objects:  96% (29/30)\rUnpacking objects: 100% (30/30)\rUnpacking objects: 100% (30/30), 12.56 KiB | 1.57 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 . Install Dependencies"
      ],
      "metadata": {
        "id": "icQpVjNNSOwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "5WTQ0wWeSluR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 . Define Functions"
      ],
      "metadata": {
        "id": "4sIWxHiNSqp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "r5GHh00xSyjD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    # set maximum input size\n",
        "    max_input_size = 4096\n",
        "    # set number of output tokens\n",
        "    num_outputs = 2000\n",
        "    # set maximum chunk overlap\n",
        "    max_chunk_overlap = 20\n",
        "    # set chunk size limit\n",
        "    chunk_size_limit = 600 \n",
        "\n",
        "    # define LLM\n",
        "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        " \n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "    \n",
        "    index = GPTSimpleVectorIndex(\n",
        "        documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        "    )\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "def ask_ai():\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    while True: \n",
        "        query = input(\"What do you want to ask? \")\n",
        "        response = index.query(query, response_mode=\"compact\")\n",
        "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
      ],
      "metadata": {
        "id": "U5Q3Z5iPWlSO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 . Set OpenAI API Key "
      ],
      "metadata": {
        "id": "kPYLj7oNUuel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste your OpenAI key here and hit enter:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuDmGjcUU70Y",
        "outputId": "1c2b8275-2b3d-4df7-e911-bc1f15f3dddc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI key here and hit enter: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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 . Construct an index"
      ],
      "metadata": {
        "id": "vUMtdTraVWvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.\n",
        "\n",
        "Notice: running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment."
      ],
      "metadata": {
        "id": "am7RkDi9VowT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "construct_index(\"/content/Chatbot_ChatGPT3_Custom_dataset/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urv2TKANVqUK",
        "outputId": "12fe31ec-2b21-4c03-f7e3-d8ddd18d0cfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x7f0d7db14850>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 . Ask questions"
      ],
      "metadata": {
        "id": "X4nt9bJqVxwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_ai()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "gd9eiRa2W9xA",
        "outputId": "e50c4296-aa8d-4926-ff53-89dbdb9b2240"
      },
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/markdown": "Response: <b>\nPeople like cooking at home because it allows them to relax and unwind after a long day, it's healthier and more affordable than eating out, they can customize their meals to their family's preferences, and it's a great way to spend time with their kids in the kitchen.</b>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Response: <b>\nYes, I always eat breakfast at home.</b>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Response: <b>\nI usually make something simple like avocado toast or a smoothie, or I'll cook up some eggs with veggies. It usually doesn't take me more than 10-15 minutes.</b>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Response: <b>\nSure. I usually start by prepping any vegetables or proteins I'm going to use. Then I'll start cooking everything in different pots or pans on the stove. I try to keep things simple and healthy.</b>",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-73fd53e5fb33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mask_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-795fcbfd0532>\u001b[0m in \u001b[0;36mask_ai\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTSimpleVectorIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What do you want to ask? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"compact\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response: <b>{response.response}</b>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEKNPDG4XDEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}